Mobile augmented reality for environmental monitoring Eduardo Veas • Raphaël Grasset • Ioan Ferencik •

In response to dramatic changes in the envi- ronment, and supported by advances in wireless network- ing, pervasive sensor networks have become a common tool for environmental monitoring. However, tools for on- site visualization and interactive exploration of environ- mental data are still inadequate for domain experts. Current solutions are generally limited to tabular data, basic 2D plots, or standard 2D GIS tools designed for the desktop and not adapted to mobile use. In this paper, we introduce a novel augmented reality platform for 3D mobile visuali- zation of environmental data. Following a user-centered design approach, we analyze processes, tasks, and requirements of on-site visualization tools for environ- mental experts. We present our multilayer infrastructure and the mobile augmented reality platform that leverages visualization of georeferenced sensor measurement and simulation data in a seamless integrated view of the environment.

 accurate as the physical observation. Therefore, we identify a gap between the environment as observed on-site and its digital representation, a dissociation that the scientist likely needs to solve for comprehending the situation. In fact, intrinsic characteristics of data, its sheer vastness, multi- dimensionality, and high spatial distribution contribute to make situation assessment one of the most demanding tasks, both for the user and for the platform [29].
Mobile visualization offers a solution by directly placing heterogeneous datasets at different spatio-temporal scales in their accurate spatial and temporal context. Moreover, mobile visualization combined with augmented reality (AR) proposes a natural way to relate abstract content to the physical world by means of graphic overlays (e.g., simulation result overlaid onto a mountain, sensor data presented visually where they are measured).
Yet, mobile visualization of environmental data raises the challenge for mobile applications. Outdoor visualiza- tion applications have to run on devices severely limited in terms of processing power, screen space, and network bandwidth. They must do so without compromising ergo- nomics or visual performance, while dealing with factors inherent to outdoor situations, such as bad weather condi- tions and rough terrain.
The contributions of this article are manyfold. We introduce the concept of mobile environmental monitoring using handheld AR. We analyze and describe its associated workflow following an iterative user-centered design (UCD) process. Thereby, we propose an innovative infra- structure that enables access to a wide range of different wireless sensors networks, allows close to real-time data access and directly integrates simulation results. Thereaf- ter, we concentrate on seamlessly integrating multivariate visualizations with the real-world context. The process is validated with experts throughout iterative evaluation of development prototypes.
2 Related work
Interaction with sensors is an active research area in ubiquitous computing. Paxton and Benford [23] evaluated children experiences in participatory sensing. Kim and Paulos [13] analyzed behavior changes resulting from the availability of indoor air quality measurements.
Visualization is a frequently used technique for ana- lyzing scientific data, well accepted within geoscience disciplines [22]. The heterogeneity of data has brought forward a vast number of techniques to deal with various issues, including spatial and temporal aspects, different view types (region-based or station-based), or various ways of relating data (2D maps or 3D globes). The study by Nocke et al. [22] stresses the importance of spatial
reference of data. As users with different levels of expe- rience and interest access geoscientific data, there is no uniform type of visualization. Most users apply standard 2D presentation techniques, predominantly for scientific purposes. Nonetheless, both scalar- and vector-based 3D data representations are an active field of research, increasingly gaining user acceptance.
Mobile GIS combines GIS software with handhelds equipped with GPS and network access. They predomi- nantly use map-based representations, similar to their desktop counterparts. A large part of the current solutions (like the Arcpad system [6]) are unfortunately single-user, limited to 2D representation, lack support for real-time (up- to-date) data or are limited in terms of data interoperability or their visual combination.
While well-known examples (e.g., [17]) can be found in virtual reality (VR), AR for geoscientific visualization is a largely unexplored with a few exceptions. King et al. [14] used a tripod-mounted AR system to visualize GIS data for viticulture. White and Feiner [32] explored unconventional visualizations of urban pollution levels using mobile AR. Their system limited to static datasets of a single 1D da- tatype (CO2). In contrast to these projects, we present a fully general infrastructure that addresses the cycle of mobile monitoring, and leverages visualization of hetero- geneous, static, or dynamic multidimensional data.
We ground the design and development on a range of human factors studies and usability experiments performed in collaboration with specialists in the field.
3 Mobile environmental monitoring
This section looks at environmental monitoring, analyzing characteristics inherent to the task from the perspective of the professional in the field. The process of environmental monitoring using mobiles was studied in the frame of a three-year project following a user-centered design approach. Thereby, a large group of end users took part in incremental, iterative phases (participatory design [24], around 65 users with various backgrounds), and focused on creating a thorough understanding of the context of use (contextual design [3]). We start by identifying groups of stakeholders and, based on their goals and activities, the context of use.
3.1 User groups and goals
Built on a hydrological context of the water cycle model, two user scenarios were selected, targeting wet-snow ava- lanches and watershed modeling. Stakeholders involved in monitoring these events and their activities can be classi- fied as shown in Table 1. Each user group has particular
1 3


Table 1 Classification of stakeholders based on their activities
  User group
Regional authority, water project manager
Environmental company/watershed contractor
Researcher
Private companies with infrastructure in the Alps
Profession and practice Engineers and specialistsa
Environmental engineersa Environmental scientista
Entrepreneur, various specialists (civil, electricity, . . .)
Main activities
Designing and managing monitoring network
Decision making and management of natural disaster
Design mitigation/restoration plans
Administration of resources
Supervision of construction
Deploying long-term monitoring system
Multidisciplinary field work, counseling
Model development and validation
Designing, building, and maintaining infrastructure
Responsible of security related to their infrastructures
  a Hydrologist, biologist, geologist, geographer, forestry
reasons to monitor the environment, mostly dictated by their activities. For example, environmental scientists aim at creating mathematical representations of a particular phenomenon (physical models), defining the inputs that trigger certain environmental conditions. Conversely, a company or government institution tries to identify envi- ronmental changes and obtain aids for decision making. Nonetheless, these goals often entail relying on the findings and understanding of others. An implicit goal is to create a shared understanding of the environment and discuss potential solutions to problems found. On-site actions should aid in more closely connecting captured process data with its actual context, the environment itself.
3.2 Tasks
Environmental monitoring can be defined as the process of continuously observing and regularly measuring environ- mental parameters of a specific area in order to understand a phenomenon. Mobile (on-site) monitoring comprises all activities conducted in the field. It does not replace but complements monitoring and data analysis in the office.
Monitoring comprises several tasks closely intercon- nected in work cycles, alternating visits to the field with work at the office. Upon identification of a suitable site for monitoring, the data pipeline is set up (i.e., acquiring legacy data, creating, placing and maintaining sensors, preparing storage, and network). Site visits serve the purpose of maintaining infrastructure, gathering samples and personal observations later examined at the office, or comparing simulation results with reality in a decision process. The latter reflects the management of environmental processes, where plans are thought out and potentially enforced.
3.3 Challenges
Environmental monitoring entails several challenges, some of them with direct impact on mobile activities.
3.3.1 Real-world conditions
Weather conditions in remote areas pose a serious chal- lenge, as consumer electronics are prone to failure in extreme conditions. This affects sensors, but also devices brought by the scientist. In some cases, sensors can be conditioned to work in extreme conditions, whereas in others they have to be removed and redeployed following seasonal patterns. Restricted accessibility locations incur high mobility costs. The challenge is to make the most out of each visit, reducing the need to return due to unexpected events.
3.3.2 Data (-source) heterogeneity
Environmental data sources vary in dimensions, periodic- ity, and other characteristics. Sensors usually produce unidimensional data, with exceptions (e.g., wind direction). However, the periodicity of measurements varies widely from fractions of a second to hours or days.
Numerical processes can output estimates of measure- ments covering an area (i.e., interpolations) or predictions of future states of the environment using complex physical models (i.e., simulations). The output format varies (e.g., single value, ranges, 2D, or 3D), and the frequency is subject to the input and its complexity. Simple interpola- tions run in fractions of minutes, while simulations can take days.
1 3

Legacy data such as plans, maps, and 3D models (DEM, DTM) are not updated frequently, and their representation is only partially faithful (e.g., DEM does not represent man-made structures or temporal characteristics like snow cover or vegetation). The scientist often complements these data with photographs that capture up-to-date information. A noteworthy, problematic aspect of spatial data is that of geographic scale. Finding the right pixel size is often a complicated task [9]. An AR overlay of spatially distrib- uted environmental variables supplies valuable information to assess pixel size or even to suggest a scale for a phenomenon.
3.3.3 Data acquisition
Acquisition involves stages from when data are gathered until they are delivered to the user. These stages vary for different data types. Automatic sensors are deployed in the required density to gather frequent measurements. Con- versely, data from manual sensors are collected at the locations of interest, pre-planned or identified ad hoc by the scientist, and uploaded later.
All data must undergo sanity and quality checks to insure validity, and security mechanisms enforcing privacy restrictions. Data may then be stored and indexed for future access. Legacy sources must be obtained from a central repository and are not indexed.
Numerical processes can be triggered automatically upon availability of input or manually depending on the model. For automatic processes, it makes sense to store and index the output. Complex simulations require powerful dedicated servers, each with different ways to specify input and access to results. In remote sites, the infrastructure must account for network availability and latency, while allowing seamless storage and retrieval of data with dif- ferent characteristics.
3.3.4 Interpretation
Numerical data are converted to visualizations that, based on perceptual abilities of people, provide an advantage to
detect patterns, differences, connections, or similarities in numerical data [26]. Typical visualizations for environ- mental data include simple and combined plots for point data, interpolations of measurements from several sensors, color-coded 2D diagrams, or maps (Fig. 1). To form a clear picture about the situation, the scientist needs to assimilate all these data that differ in dimensionality, update rate, and representation. Often, this process entails switching between representations (plots, visualizations, numbers) and correlating with the real world (maps, models, photo- graphs) to understand where effects originate.
4 Approach: in-context visualization of environmental data
The ultimate goal of mobile environmental monitoring is to visualize abstract data, such as sensor measurements and simulation results in the context of their occurrence. Two general concepts, context and situation, underlie our approach.
4.1 The notions of context and situation
While monitoring events on-site, users operate within a certain context, entangling the various participants, the actual environment with its artifacts, and other high-level conditions, such as weather or noise. Context and situation are two overlapping concepts that refer in general to the action space within which a user operates. We are inter- ested in situation awareness, which encompasses user understanding of the current action space. Context is important to consider the characteristics of the action space: physical processes are being observed that originate in an environment, which may be heavily under change. This notion is also the crux behind the infrastructure pre- sented in this article: visual representations of environ- mental data are brought into direct context of the actual environment.
The spatiotemporal characteristics of the environ- ment are of utmost importance for creating a correct

  Fig. 1 Visualizations. Left combined plot comparing measurements of snow height and temperature over time. Middle left color coding of soil moisture values. Middle right interpolations for surface skin temperature. Right color coding of land use
1 3


 understanding: bringing representations of physical pro- cesses in relation to spatiotemporally outdated or only partial representations of the physical environment may lead to the wrong interpretation of the situation. Within the boundaries of update rates, data always refer to the latest spatiotemporal stage of development of this environment. Assuming that the office is not in the direct vicinity of the environment being observed, this separation may well lead to misinterpretations when environments are under heavy change or only limited representations exist. Ideally, we want users in the field to obtain the latest sensor data without manual intervention. All this must happen in a collaborative framework whereby users can interact, communicate, and discuss findings and potential solutions.
Hence, our aim is to create (a) a correct per-user understanding of the data representation in relation to the actual environment in its latest spatio-temporal state, as well as (b) a shared understanding of the knowledge gained by users with potentially different backgrounds and dif- ferent perspectives on the site.
4.2 Visualization using mobile AR
The method of choice, augmented reality, strives to render computer-generated artifacts correctly registered with the real world in real time. The term ‘‘correctly registered’’ means that these artifacts appear in the correct position relative to the point of view of the user. We propose a mobile AR application for visualization of environmental data in the context of the site of study. This in-context interactive visualization supports the scientist in monitoring tasks, from displaying sensor positions for deployment or current readings for maintenance, down to integrating representa- tions of multivariate data and complex simulations. With AR, complex datasets are contextualized within the envi- ronment they originate. Similar to a multilayer approach, multivariate data can be compared and analyzed through a combination of representations (mixed dimensionality).
To support situation awareness, we extended in-context visualization with a shared view infrastructure, deploying and sharing footage from multiple imaging devices. This improves understanding of spatial relationships and pro- vides a solid basis for cooperative work.
4.3 Requirements
The aforementioned challenges outline requirements for a mobile AR system that necessarily differs from general mobile applications and from in-office GIS systems. The main requirements include:
Georeferenced data: all data gathered must be georefer- enced to generate in-context visualizations.
3D digital model: terrain models are needed for visual- ization and simulations.
Network access: although it is possible to use prere- corded data, the full potential of the application comes with real-time sensor/simulation updates and communi- cation with other users.
Accurate tracking: high accuracy estimation of the user’s view (pose ? video) require high-quality sensors such as camera, orientation sensor, and GPS.
Interaction: visualization and user interface should be clearly visible, preferably show undistorted colors, support correct depth perception.
Robust mobile system: the mobile platform needs to be robust, ergonomic, and have an outdoor readable display. The interface must allow usage under extreme conditions (e.g., wearing gloves).
5 System infrastructure and workflow
Our infrastructure is divided in five main components, each presenting clear interfaces to distinct services (see Fig. 2). The design relies on a dataflow model for communication and interoperability. Our system leverages on-site/off-site communication and data exchange, supported by a network layer underlying system components. Sensor and acquisi- tion components manage sensor measurement, filtering, and aggregation. Furthermore, simulation and analysis components furnish additional scientific datasets that can be used on-site. The mobile support is divided in deploy- ment, providing infrastructure to access data—environ- mental and other— and prepare it for rendering, and the run-time client, providing visualization and interaction capabilities.
5.1 Network layer
Our system follows an heterogeneous configuration of wired and wireless network. Internet and LAN networks provide off-site access to sensor information such as database access or web portals, while wireless links support sensors and the mobile interface.
Interactive mobile monitoring requires a network with high availability and real-time refresh rates. We rely on the mobile phone network (GSM) where available. A special, low powered WiFi bridge was developed for remote areas, where connections are degraded or entirely absent. It operates either in a multi-hop setup, using more than one link to extend WiFi connection to the sensing location or connected to the mobile data network in an area of good reception. Links are mobile and can be set up in a relatively short amount of time.
1 3


   Fig. 2 Overall system diagram. Component blocks are organized according to their deployment (i.e., on-site, remote). Icons at the top right of each block indicate the (usual) platform running the components (i.e., dedicated sensor station, distributed systems, mainframe, laptop, and handheld device)
5.2 Data acquisition and management
This component encompasses sensor measurement, trans- mission, and low level data management (physical and logical data services). Our system unifies hardware and software aspects of sensor acquisition and management, offering the possibility to plug-in various sensor networks (e.g., Pachube, OGC SWE1 or ad-hoc solution [33]).
The current implementation relies on the Global Sensor Networks (GSN), a distributed sensor network middleware, developed to provide uniform, ubiquitous interface to a large deployment of varied sensor types (i.e., mechanical, electrical, thermal, digital/analog, simple, or multivariate) [1]. Similarly to OGC SWE standards, GSN supplies a multilevel architecture (implemented in Java), offering the possibility to describe, aggregate, or filter sensors.
GSN stores measurements from automatic and manual sensors, it employs parametric and probabilistic methods for quality control and supplies federated access to data [12]. It offers temporal and spatial queries, as well as dynamic registration for push/pull data retrieval. It also processes plot queries, resulting in 2D graphical content, avoiding transfers of a high volume of data over potentially expensive network links.
GSN uses extensible markup language (XML) for data exchange and interoperability. We use this format to communicate and interact with sensor measurements and between the components of our infrastructure.
5.3 Analysis and simulation
Collected and filtered sensor measurements serve as input to data analysis and simulations. These components and subsystems generate advanced datasets that can be used and visualized on-site.
1 http://www.opengeospatial.org/standards/swes. 1 3
A generic library aids in the conversion of sensor data to scientific analysis tools such as Alpine 3D (A3D) [16] and Matlab. The library MeteoIO2 presents a uniform, format- independent interface to integrate meteorological data in an application. Based on plug-ins, it enables easy access to different data sources (e.g., GSN, A3D, and GeoTop) and hydrological simulations. MeteoIO performs interpolations of meteorological parameters and includes plug-ins to access simulations in real time (e.g., GeoTop [27] and Snowpack [18]). To run a simulation, the library obtains all the parameters and the specification of data (GSN nodes, data types, etc.) from a file. MeteoIO then retrieves data from GSN and feeds it to the appropriate model via a plug- in. Upon reception, results can be converted to other for- mats (transcoded) using filter plug-ins, hence deriving the image formats later used by the mobile client.
5.4 Deployment
To implement our awareness concept, we introduced the notion of on-site campaign services deployed to support on-site activities, in particular collaboration. During cam- paigns, different services run on a portable computer at the site of study and provide access to shared information (e.g., remote views and annotations).
A shared view service (SVS) supports the notion of multiple perspectives on the environment. It allows users to look through the ‘‘eyes’’ of a collaborator. The SVS also shares views from pan-tilt and infrastructure cameras, enabling remote observation of the location. To this end, SVS can connect to GSN to retrieve images from cameras deployed as part of the fixed sensor network. Moreover, it can relay images to GSN. Finally, the SVS can be extended to record georeferenced annotations linked to snapshots from mobiles.
2 http://slfsmm.indefero.net/p/meteoio/.
                                                                                                                                                                                           

                                                                                                                                                                                  Fig. 3 Block diagram of the mobile client
The SVS is based on a publish/subscribe model. A handshake protocol acquires identity of clients and infor- mation about the camera lens system (i.e., intrinsic parameters). Thereafter, clients regularly supply tracked frames (i.e., jpeg image ? 6D position). Clients can sub- scribe to frames from any camera. As the SVS is deployed on a dedicated computer, we avoid network overload and latency compared to running directly in GSN. The SVS uses its own on-site ad hoc network (private WiFi) guar- anteeing close to real-time performance (dependent on frame update rate and resolution).
As our on-site computer is connected to GSN, a data service can cache incoming sensor/simulation data for all users, reducing outbound traffic. This component leverages data conversion to assure interoperability with the client. Incoming georeferenced data undergoes geometric con- versions, transforming its reference frame to that of the application (i.e., projection to WGS84–UTM and reference adjustment). Furthermore, 3D simulation results are trans- coded, obtaining geometry as a regular polygonal mesh and 2D georegistered overlays (e.g., textures) with corre- sponding coordinates. These conversions occur in accor- dance with the data format supported by the client and considering the mobile platform capabilities (downscaling large polygonal models, downscaling image content, etc).
5.5 Mobile client
The mobile client defines the interface for end users. It delivers access to the data on-site, as well as visualization, interaction, analysis, or reporting.
5.5.1 Hardware platform
The target platform is a Panasonic CFU1 tablet PC (Intel Atom CPU Z520 1.3 GHz, 1 GB, Intel GMA500). Although its processing power is relatively limited, the platform is ideal for outdoor, rough environments: it is ruggedized, waterproof, has a long-lasting battery life complemented with two hot-swappable batteries, and a 5.700 touch screen. The system is complemented with a UBlox differential GPS, an InertiaCube3 or XSense tracker, and a UEye camera with a Pentax 4.2mm wide angle lens.
5.5.2 Software platform
Our client is built on a specifically designed mobile infrastructure, KAOS, which supports multi-threaded plu- gins and an event-based communication mechanism. The KAOS infrastructure has been optimized and developed to handle low level mobile and portable devices.
Above the KAOS framework, we implemented our specific mobile AR/VR client (see Fig. 3), handling graphics rendering (OpenGL), graphical user interface, AR registration, and audio communication (Skype API). We made prolific usage of the plugin mechanism to support a variety of tracking technology, but also for handling dif- ferent types of datasets. For tracking, we developed a standard inertial?GPS plugin as well as an advanced hybrid tracker. We developed in parallel a simulator/ tracking recorder leveraging the possibility to debug and test the interactive platform off-line.
6 Interactive tools for mobile visualization
Our application workflow finds its foundation in Shnei- derman’s visual information seeking mantra: overview first, zoom and filter, then details on demand [26]. These foundational tasks are orthogonal to the functional activity groups afforded by the application: data exploration, view management, and collaboration. Thereby, users activate visualizations in overview mode, zoom to create a specific focus, or deploy filters to select what data are visible. Additionally, users can request details where needed.
6.1 Data exploration tools
Our platform offers a variety of 1D, 2D, and 3D visuali- zation techniques that can be manipulated in AR or in VR mode. Each technique has been adapted to the types of data available. Here, we focus on the two main data sources: sensors and simulation results.
Sensor measurements are represented with labels spa- tially registered in 3D at the geolocated position. To account for clutter when simultaneously visualizing a large number of labels, we developed alternative formats and filtering techniques. A minimal, iconic format indicates only the position of stations. An informative format shows also names, while an extended format shows each label
    1 3


  Fig. 4 Sensor visualization: different presentation modes. a Labels with station name. b Label with station name and one observed variable. c Icons. d Icons with spatial filtering from station id
with a single reading for a common data type (filter by data type, e.g., temperature), see Fig. 4. Tapping a label or icon brings up a summary table of associated sensor types and their observed values. Stations can also be filtered spatially by browsing through a spin menu, thereafter only pre- senting the selected station in the AR view (see Fig. 4d). Additionally, a plot mode displays graphs comparing measurements of multiple sensors for a single station or across stations (provided by GSN).
A range of simulations and simple interpolations are available as overlays on the video. The domain expert can filter and modify the type of simulation by browsing through a spin menu. Currently, only one simulation can be displayed at a time. Multi-modal visualization is supported by combining overlays with sensor visualization. For example, by displaying a surface skin temperature inter- polation overlay with solar radiation station measurements, one can analyze the effect of solar radiation.
The choice of AR brings about several perceptual issues that plague AR visualizations [15]. In particular, mobile AR is affected by depth distortions related to incorrect depth interpretation of virtual objects and visibility issues related to screen problems and physical conditions of outdoor environments.
In our case, when activating an overlay, the 3D terrain is rendered from the perspective of the video camera, but the terrain model spans a wide area, most often occluding a large portion of the video. The literature on mobile AR in these cases uses simplified wire-frame models [11], suitable to
convey structure of buildings and urban models. In the case of environmental monitoring, 3D models often represent only elevation and lack structures. Additionally, to make sense of the situation, the user needs to see the color mapping.
As a solution, the user is given control of the opacity with which the terrain is rendered. The terrain can then be viewed in full color or semitransparent allowing to view the real world underneath. Thereby, a user can see trees on a slope or a barn next to the river, where the digital model just displays a flatland.
Nonetheless, a 3D terrain flatshaded with artificial colors offers particularly poor depth cues, even more so when rendered semitransparently. To address this visual issue, during transcoding, we create contour line models using an algorithm proposed by [5]. The terrain can be rendered in line mode, with the overlay textured onto the contour lines, or in fill mode, showing both lines and polygons. The line mode displays colors overlayed on the lines, while the video background is clearly visible. Isolines provide additional depth cues, as they become denser in screen space with increasing distance (see Fig. 5). Depth ordering is controlled by the polygonal 3D model, it must be rendered even when invisible to compute occlusions in the graphics pipeline.
6.2 View management tools
Visual information search is relatively restricted by tradi- tional AR, particularly concerning ways of gaining
1 3


  Fig. 5 Terrain overlay visualization. Left terrain in fill mode with transparency. Note how the transparency shows the real world underneath while preserving the color coding of the overlay, and
isolines provide a sense of depth. Right overlay mapped on isolines. The lines give a general idea of the color distribution while clearly showing the video
overview or zooming into portions of the dataset. To change the viewpoint in a traditional AR setting, the user has to move to the desired position. Visual exploration can greatly benefit from quickly accessing different perspec- tives in the information space with reduced physical movement.
To complement AR visualization, we developed an interactive view management interface based on the SVS described previously. Our goal is to harness the expres- siveness of the real world captured by imaging devices and make it part of the dataset. Thereby, the workflow is complemented with virtual views over the dataset provid- ing overview, and real camera footage from imaging devices for zooming, spatial filtering and details. We describe the interface and advantages of multiview man- agement in [30], where we also consider integrating mul- tiple perspectives in a single image.
Making sense of data requires understanding how it relates to the real world, creating a picture of the situation, and maintaining it as new facts from real-time data become available. In [31], we analyzed navigation techniques that allow a user to build a mental model of the situation from multiple cameras with little effort.
Overall, the multiview system was well received by end users in different evaluation stages (see Sect. 7). This system plays a role in visualization interaction, but it also forms the backbone for collaboration metaphors.
6.3 Collaboration tools
Collaborative tools in our outdoor AR system cover mul- tiple one-to-one on-site collaboration, on-site to off-site collaboration (both asynchronous or synchronous). The SVS described above and its interface [30] offer a shared awareness model between participants of an interdisci- plinary team. Users are aware of other users’ locations, their
viewpoint and the content of their view (i.e., device’s screen).
These features are extended with communication-cen- tered tasks. Users can toggle small overlaid presentations of other users to locate the whereabouts of team members, and they can initiate communication sessions using pre-config- ured voice channels. The latter is particularly useful when collaborating in distributed fashion with mobile or remote peers. Combining view sharing with voice communication helps to generate a shared awareness for this kind of task.
Asynchronous collaboration methods build around the notion of georeferenced annotations, possibly containing graphical content (e.g., a screenshot). They encompass several ways of noting down remarks, by using semantic text-based annotations, voice notes, and marking tools. Semantic annotations are ordered logically according to the application, such as sensor failures, or specific observatory findings. A set of simple marking tools can be used to draw on screenshots. An example of a marking task is taking a picture of a snow profile and marking the various layers that can be observed.
7 Validation and discussion
This section presents the methodology, the different application scenarios we explored along the duration of the project, and the lessons learned. Two different scientific areas were targeted for our case studies: snow sciences and hydrology.
7.1 Methodology
We used different evaluation methods and UCD tools to validate our system as shown in Table 2. The process followed a systematic approach, where top–down analyses
1 3

Table 2 Evaluation methods used in different phases of the development process

  Modality Prototyping cycle
Expert workshop
Showcase interview
Demo SLF Formal evaluation
Demo CHI2011
Development phasea Specification and design
Prototype 1
Prototype 2
Dissemination Prototype 3
Dissemination
Tool
Meeting Survey Interviews Walkthrough Subjectivec
Usability Subjective
Showcase Usability Subjective
Showcase
Participants 52 experts
52 experts
8 experts
Field experts 29 experts
HCI experts
Resultb
System workflow Validated design
Device weight (-)
Data selection (-)
Label readability (?)
Data selection (-)
Overlay readability (?) Annotation and multiview (?) General feedback
On-site data access (?)
Interpreting AR sensor readings (?) Interpreting AR overlays (?) General feedback
  a Prototypes 1 to 3 as shown in Fig. 6
b (-) Identifies negative results, (?) identifies positive results, details in text
c Subjective evaluations used structured questionnaires followed by short discussion
helped pinpoint issues, subjected later to bottom–up studies in the frame of the proposed solution. Experts participated in this user-centered development approach with different levels of involvement. A group of ‘‘partner’’ experts (15 people, representing groups in hydrology, sensor technol- ogy, and sensor networks) participated closely in every stage of development. They helped to select a second group (16 people, stakeholders from government entities and companies), appointed as ‘‘advisory’’ experts, who partic- ipated sporadically at specific development milestones.
7.1.1 Prototyping cycle
A definition phase in the first 6 months of the project involved numerous surveys, interviews, and discussion with a large group of experts (52 people) outside our partner group. These sessions helped to establish the role of mobile AR in monitoring activities and to define
conceptual tools. Conceptual tools were studied and vali- dated from a human factors and ergonomics perspective, but also to analyze phenomena from deployment sites, and conceptually redefined where appropriate.
7.1.2 Expert workshops
Two expert workshops were scheduled to showcase the first functional mobile AR prototype (Fig. 6a). Firstly, in Davos, Switzerland, 12 end users from research and envi- ronmental organizations were introduced to mobile moni- toring concepts, scenarios, and the prototype. A two-hour (plus) intense discussion followed the presentation. Sec- ondly, as part of a large hydrology workshop in Lahti, Finland, 40 end users from research, government/munici- palities, and companies went through workshops in groups of 3–5 people. After a brief introduction, each group took 10–15 minutes to collect thoughts and notes for a longer
 Fig. 6 Prototypes overview. a The first prototype evaluated during expert workshops displayed sensor label and a wireframe DEM. b The second prototype, showcased in expert interviews, enabled selection from a variety of sensors/simulations, showing line or fill
overlays. c The third prototype, used in the formal evaluation, applied a workflow-oriented UI to organize visualization activities, and introduced transparent overlays, multiview, and collaboration tools
1 3

Pers Ubiquit Comput
discussion. Feedback from the workshops helped refine, and in cases redefine, tools and components of the system.
7.1.3 Showcase interviews
The prototype in Fig. 6b was subject to showcase inter- views. During the interviews, participants were demon- strated the main features of the system, asked to explore them and we collected their feedback through an unstruc- tured interview as well as a questionnaire. Additionally, seven experts from snow science with varying backgrounds (some regular site visitors, others mainly office-based) were also interviewed.
7.1.4 Formal evaluations
Formal evaluations focusing interpretation of AR visuali- zation and on-site access to data were carried out in two application case studies described in Sect. 7.5.
7.2 Case studies
Two long-running scenarios were chosen as a test bed for our system. The snow science scenario was tested in Davos (Switzerland) in the context of snow avalanches (in col- laboration with SLF and EPFL3). The hydrology scenario was explored with Aalto University in the context of watershed modeling in Kylma ̈oja (Finland).
7.2.1 Snow avalanches in Dorfberg, Davos
Context: Wet-snow avalanches are significant, frequent hazards in mountainous regions, which have a large degree of potential damage to infrastructure and residents [20, 21, 28]. Nevertheless, the formation and triggering of these avalanches are very complex and poorly understood [2, 25]. The Dorfberg is a steep southeast facing slope above the city of Davos (Grisons, Switzerland), which is an ideal study site for wet-snow avalanches. The scenario aims at improving the understanding of wet-snow avalanches and their related processes. The Dorfberg site provided a real- case scenario to test the applicability of the system described in this paper in remote terrain and rough weather conditions.
Implementation: The site has been equipped with numerous automatic meteorological sensors including a full weather station (e.g., temperature, wind speed, solar radiation, and snow depth), and several sensors to measure snow cover and soil characteristics like snow temperature or soil moisture content. The site is observed from different
3 E ́cole Polytechnique Fe ́de ́rale de Lausanne.
perspectives with time-lapse photography, and researchers visited the sited regularly to perform manual field mea- surements and to maintain the sensors. To provide rea- sonable network coverage, a multi-hop WiFi link was deployed and all sensors have been integrated into GSN. Using the sensor data as input, Alpine 3D, an advanced spatial numerical model [16] was run in order to produce spatial information of the interaction between meteorology, terrain, and snow cover. Moreover, simple spatial inter- polation methods were applied for the data. Sensor data and simulation output could be queried by the platform in near real time, and the variable could be displayed while being on-site.
7.2.2 Watershed modeling in Kylma ̈oja
Context: The catchment of Kylma ̈oja is located in the south ofFinland.TheareaisdrainedbyKylma ̈oja,astream formed by the merge of three branches close to its geo- metric center. The catchment features clayey soils that closely follow the valleys, with a topography dominated by moraines structured as shallow layers on short rocky hills. The site scenario is a pond and its neighborhood, spanning 391 by 390 m, and yielding a surface area of 15.24 h. The land use in the site is highly heterogeneous ranging from grassland to forest and containing natural and man-made features.
Implementation: Sensors were deployed in key points along the Kylma ̈oja stream, spatially arranged to optimize their representativeness (drained area). On-site campaigns aimed at the collection of data representing specific qual- itative (turbidity, conductivity) as well as quantitative (water level) hydrological parameters. The parameters selected for sampling can be used for several hydrological models and are simple enough to be understood by non- experts when evaluating and using the system. The sam- pling was performed using commercial Luode water sensor stations equipped with a submersible optical YSI-600 water quality sensor and a pressure gauge.
The 2D hydrological model used for the campaign was r.sim.water, a spatially distributed, dynamic 2D hydro- logical simulation model based on the continuity equa- tions solved by Green’s function Monte Carlo method [19]. The simulation is based on a 70 mm/h rain event and a value of 0.13 for Manning’s coefficient to account for the effect of land use. The model was run with a ten- minute time step for a period of an hour. To accurately represent the site, an average value obtained through empirical evaluation of 30 cm was added to the final water level to account for the depth of the pond in raster cells located inside the pond. The simulation results consist of six GIS rasters representing predicted water level at the requested time.
Author's personal copy
  1 3

7.3 Prototyping cycle
The foundation of our work, summarized in Sect. 3, was outlined in the definition phase. During this phase, we created mock-ups that experts refined into the initial con- cepts for mobile monitoring.
Figure 7 shows examples of visualization concepts. The tool in Fig. 7a draws miniplots of accumulated sensor values in AR. The concept was abandoned due to the lack of support in the early pipeline, and after initial testing, the screen form factor (outdoor lighting, resolution) proved that it would be difficult to interpret the values. Similarly, the tool to visualize sensor measurements in Fig. 7b was refined to reduce screen clutter by showing at most one measurement per station at a time. An initial idea to display several overlays for comparison (e.g., progression of a simulation) was also sketched (Fig. 7c). Complexity to implement this concept on our platform led to discontinue its development. We re-introduced it later through a tem- poral approach: the model for the Kylma ̈oja catchment produces a series of overlays that are animated in place, but they cannot be shown concurrently. Fig. 7d shows con- ceptual overlays as finally implemented in our system.
Besides developing conceptual tools, experts also helped us specify tasks where mobile technology is foreseen to have important impact, such as: sensor deployment and mainte- nance, monitoring and understanding environmental pro- cesses, communication, and collaboration and management
of environmental processes. These outcomes set the cor- nerstones to define test scenarios where the technology could be gradually deployed and tested (Fig. 8).
7.4 Expert workshops/showcase interviews
Overall, experts confirmed that our mobile monitoring approach complements effectively current practice. Our system workflow was noted useful for sensor setup and maintenance, ‘‘real-time feedback for sensor setup or manual measurements insures that the data make sense, saving time and avoiding returning to site for new mea- surements’’. It was also found useful as information exchange for decision-making support when multiple users with different perspectives need to discuss a situation.
7.4.1 Usage scenarios and applications
The discussions brought up an extensive list of potential scenarios for on-site monitoring. Experts regarded the case studies as good, representative examples. Lessons learned from them can be transferred to scenarios with akin situa- tions elsewhere, for example, transport of pollutants in streams. Of particular interest are situations that can be reproduced in the field by visualizing results of the model, showing how the modeled situation affects the environ- ment (e.g., effects of rain, such as flooding, can be observed even if it is not raining at the moment). Another

  Fig. 7 Conceptual design for the visualization. Top sensor represen- representation. c A series of 3D overlays to show temporal tation. a Miniplots show data distribution of sensor over time. measurements. d 3D spatial overlays mapped on the terrain
b Labels present observations from all the stations. Bottom overlay
1 3

  Fig.8 Scenariomatrix.Sites,sensors,demos,andvisualizationsfortwoscenarios.ToprowDorfberg,Alps,Switzerland.BottomrowKylma ̈oja, near Helsinki, Finland
noteworthy use case in management is on-site visualization in conjunction with warning systems, for example, flood, avalanche, or landslide warnings. In general, on-site monitoring increases the amount of work (e.g., testing, calibration, and maintenance stages), but the benefits gained with it may well compensate (e.g., immediate notification of failures in measurements).
On a functional level, an annotation tool was deemed highly beneficial for professionals who spend most of their time in the field (employees of forestry, environmental inspectors). Participants also raised the need to integrate sensors for mobile measurement. For example, the Snow- Micropen, a manual instrument to assess the hardness of the snow cover, would certainly benefit from direct feed- back when taking samples in the field.
7.4.2 Visualization
In general, users found 3D visualization useful for those situations in which spatial datasets are the basis for anal- ysis. However, for some tasks, it is overrated. Single-point data measurements that are not further processed may fall under this category. Nevertheless, 3D visualization is beneficial for users that have difficulties reading maps. This was confirmed by all experts in Davos and Lahti: ‘‘Experts might prefer using 2D, but a non expert understands 3D visualizations more easily. When working in unfamiliar environments 3D visualization is more useful than 2D visualization.’’
Experts found the integrated visualization and compar- ison of different sources of information was a major advantage for decision making, AR was rated useful in the field, though not for all situations (e.g., during sensor setup). One situation where AR finds a major value is for showing subsurface structures or different layers of ‘‘material’’ (soil, gras, snow). Furthermore, experts stated that AR also merits use in the office, in a form of ‘‘tele AR.’’ As such, it may certainly help multidisciplinary teams of users cooperate.
7.4.3 Mobile device and user interface
Users do not always need all the features and may want to customize the system. Users stressed having a challenging learning curve with new user interfaces (UI) and expressed a preference for ‘‘single button’’ metaphors known from smartphones. The UI should be controllable with gloves, for what the keypad of the device proved to be too small. Thus, buttons on display have to be big enough. Partici- pants judged the device too large and cumbersome, par- ticularly for transport. They would rather have a modular device that could be stripped down if needed.
7.4.4 Collaboration
Experts in showcase interviews were positive about the notion and system for a shared understanding (6 experts rated very positively, one very negatively: M = 4.71,
1 3

 SD = 2.06). Annotations and marking tools to make ‘‘notes’’ on screenshots were well received and, although not devised for complex documents, they seemed good enough for reporting plans (M = 4.28, SD = 1.38). Experts noted that the tools improve office to site com- munication (M = 4.43, SD = 1.61). Results obtained with 7-point Likert scale, 7 was best.
7.5 Scenario evaluation
In the context of two deployment scenarios, this section reports results of two formal evaluations, and feedback from end users regarding experiences and lessons learned through the usage of our system in the field.
7.5.1 Dorfberg, Davos
7.5.1.1 Formal evaluation We recruited 20 specialists (age 18–75) including eight geoscientists for the evalua- tion. The evaluation focused on outdoor visualization issues (such as visibility and readability), underlying the use of AR and the general workflow. After a short intro- duction, we conducted a walkthrough of each feature, allowing users to tests specific functions. Subsequently, they completed a questionnaire (7-point Likert scale questions with 7 for high and 1 for low). An extended questionnaire was given to geoscientists. Our system was generally rated good to very good.
One main issue raised by participants was due to the screen visibility (M = 3.5, SD = 0.95) under really bright conditions during the evaluation (we addressed it later with a visor that can be attached to the device). Still, experts found the text well readable (M = 5.33, SD = 1.63), as well as acceptable image quality (M = 4.47, SD = 1.02). Furthermore, regarding visual interpretation, participants successfully located sensors in the real world from the information presented on the screen (M = 4.93, SD = 1.16) and interpreted the visual AR information with the physical environment (M = 5.47, SD = 1.07, geosci- entists score was M = 6.00, SD = 1.25). The use of AR was highly rated (M = 5.00, SD = 1.15), as was the use of 3D overlays (M = 5.88, SD = 0.99).
Participants found the system easy to use (M = 4.93, SD = 1.03) and enjoyed the experience (M = 5.21, SD = 0.92). All participants, without exception, were enthusiastic about accessing and comparing different data representations (M = 5.88, SD = 0.99). The multiview system was also well received, as already reported in [30].
7.5.1.2 Visualization pipeline Throughout the on-site monitoring deployment, the data pipeline was well received by experts at SLF. Different types of data could be queried and directly displayed in the platform, choose appropriate
display formats (1D/2D/3D). Users found it easy to control overlays of the different sensors and layers. In their opinion, such direct feedback on sensor data can be beneficial for sensor maintenance and control. The different display for- mats help get a feeling for the site, although this was not applicable to the current scenario as the users obviously knew the site very well. Switching between different view angles was not applied directly for the scenario as the site is very limited in size. Nevertheless, the different view angles proved valuable for data analysis in the office.
7.5.1.3 Workflow integration On-site data access and visualization options were noted to improve the on-site workflow with information about the actual situation. This can guide decisions toward additional action or measure- ments while being on-site. Nevertheless, on-site data analysis is still a rough simplification of the workflow and data analysis usually performed in office after field cam- paigns. Along these lines, on-site analysis only provides an overview of the situation, but detailed scientific in-office analysis and interpretations cannot be replaced. Users stressed the complexity of this scenario, where analysis is usually not straightforward, but complex and time-con- suming. Thus, most data analysis must be performed in the office. Along the development of the system, users noted the need for tools for georeferenced mapping while being on-site (e.g., directly map an observation, like an avalanche release zone or a specific layer in a snow profile on a screenshot).
7.5.1.4 Usability The system presented limitations in terms of ergonomics: it was relative bulky and large, and therefore not very convenient for field work. Moreover, handling the system with gloves was not simple and the quality of the display was limited, especially in bright sunlight. The quality of the displayed data (e.g., overlays on DTM) frequently showed significant deviation from reality. This is probably due to the low spatial resolution of the elevation models.
In the context of snow sciences and based on the com- ments of experts in this field, it can be concluded that the concept could be valuable in supporting the on-site work- flow within the limits of a meaningful applicability.
7.5.2 Kylma ̈oja, Finland
7.5.2.1 Formal evaluation The system was demonstrated to the public and to specialist end users. A formal evalu- ation was also conducted with experts following the aforementioned methodology. The evaluation of the system was performed in the last on-site campaign and two main features were tested, namely displaying sensor measure- ments and visualizing outputs of a 2D hydrological model.
1 3

 Nine participants took part in the evaluation, all experts in the field of geospatial analysis. Participants successfully matched scientific visual information with information from the real world (M = 5.83, SD = 0.75) and could easily locate the sensors (M = 5.67, SD = 1.00). With regard to accessing heterogeneous data, responses were mixed. We hypothesize that this is due to a difference in area of expertise, as users were not purely hydrology ori- ented. Registration of the data with the video scored acceptable (M = 4.33, SD = 0.52), which is probably due to the planarity of the terrain.
7.5.2.2 Platform components Professionals in the field noted the important aspect of independent and interde- pendent tasks for mobile AR environmental monitoring and how these should be integrated in the a system (e.g., data availability for running a model). Some of these tasks were technically too difficult or unfeasible with our mobile platform. As an example, the idea of building a simulation machinery to run models in real-time atop GSN was abandoned in favor of a separate web processing service for interoperability reasons. Another meaningful example in the above sense is the replacement of the hydraulic 1D model developed in-house with a 2D hydrological model due to difficulties in conducting hydraulic modeling with LiDAR-based stream geometry on very small (less then 1 m depth) streams. Hence, more flexibility and adaptability in terms of adding and supporting more scientific processes (modeling, analyzing, simulating) are both important components that should be integrated in future version of the AR environmental monitoring.
7.5.2.3 Standards Implementing specific OGC standards (WPS, WFS, WCS) would greatly enhance the usability of the mobile AR client. This would allow instant in-context retrieval, query and even editing of environmental spatial data. The client could provide complex functionality by chaining OGC services hosted on remote sites. With the envisioned developments in network transmission capa- bilities (4G), the manipulation of the payload would not be problematic anymore.
7.5.2.4 Modeling The main aim of the project/scenario was to trigger the execution of environmental models in the field near real time while feeding them sensor data. This goes beyond the concept of on-site environmental model- ing, as it encompasses both modeling and monitoring activities. Models that employ adimensional abstract parameters are going to benefit from on-site modeling because it is possible to adjust the value of the input (parameter or variable) and directly relate the change result with the AR context and reality. This facilitates association of abstract input with palpable, observable input, and thus
the role of the abstract data can be understood better. Looking further, spatial autocorrelation is a property of spatially distributed environmental variables and failure to account for it can yield false results [7]. Because the spatial autocorrelation is an abstract property, it cannot be observed in nature, but an in-context AR visualization would allow a better estimation and understanding of this property especially for non-GIS experts.
7.6 Discussion
7.6.1 Environmental monitoring
On-site visualization provides clues into and facilitates the understanding of various abstract environmental parame- ters with respect to specific natural processes. In general, end users confirmed both the technical quality of the infrastructure and the usefulness of our approach. Our mobile AR system accomplishes the goal of presenting information from an environmental sensor network in its real-world context. With few additions (modeling), it could support model validation and calibration for certain classes of environmental models.
7.6.2 3D Visualization
Not all environmental scientists had prior experience with 3D visualization, though those who did rated the tool very positively. The progress of the prototype also influenced end-user attitude. The nordic expert workshop confirmed that 3D is generally believed to be the ‘‘future’’ of hydrology/environmental visualization. During the final demonstration with a more mature prototype, a higher preference for 3D visualization was noted in general. A number of users stated that they would use the system in its current form, and others would adopt its component-based architecture. We must concede that the combination of AR and mobile monitoring entails a new paradigm compared to traditional tools. In this sense, it is reasonable that experts will need time to integrate this solution in their scientific workflow (which will imply integrating a broader range of data interoperability in our system).
7.6.3 Performance
The major limitation was the computational capability of the ruggedized handheld device: as we modified our hardware prototype throughout the project (i.e., specific casing, tracking devices, and ergonomics), adapting it to all weather conditions was really challenging, as was insuring reasonable performance computation (i.e., real-time graphics, computational steering simulation trials). We hope to address these issues in the future by redeveloping
1 3

 our system on more powerful platforms that benefit from the recent trend of advanced smartphones (e.g., iOS, Android).
7.6.4 Domain
Along the project, we started to develop more domain- dependent tools (like marking tools for snow profile), which will be integrated into a specific domain-based layer in our infrastructure. Similarly, this work was not highly focused on issues related to network reliability, data quality, data uncertainty, or simulation accuracy. More work along these lines is needed, especially regarding novel ways to combine visualizations in AR. Although some of the proposed features are quite application-spe- cific, most of our concepts and results can readily be deployed in other scientific or engineering fields where pervasive sensors are deployed.
7.6.5 Mobile sensing
The potential of mobile tools for environmental science can only benefit from interoperability with mobile sensors. As was expressed in interviews, turning the mobile AR tool into an interface for manual sensors would definitely make it worth deploying. Furthermore, approaches to continuous mobile sensing would certainly complement the current workflow. In a way, the view-sharing capabilities go in this direction, making the camera a sensor that continuously documents the experience.
7.6.6 Collaboration
The view-sharing mechanism in combination with anno- tations and voice communication gave way to an interest- ing collaboration platform. End users constantly highlighted the potential of the system for teamwork and collaboration, for example for decision making or com- bined with a warning system. As we developed the system, we noted that the mechanics of teamwork for users with such mobile systems present a level of complexity that warrants further study. In future work, we will address and extend the collaborative aspect by providing more advanced tools supporting mobile situation of experts and better quality of service (network issue, data querying errors).
8 Conclusion
In summary, we contributed the notion of mobile environ- mental monitoring and presented a thorough description of its process and workflow. We presented a novel 3D mobile
AR platform, which enables a researcher to visualize and interact with data in-context, integrated in an infrastructure covering wireless sensor acquisition/management to mobile visualization. Our solution targets real-time access to sensor data, simulation results, and the physical world, while providing dedicated tools for analysis and comparison. Collaborative aspects were addressed, by providing anno- tations, view-sharing, and audio communication.
Overall, on-site AR environmental monitoring can be regarded as a promising field. We expect that it will mature in next years and position itself among the fundamental techniques of environmentally aimed scientific inquiries.
