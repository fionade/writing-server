ABSTRACT
Explorations in the use of Augmented Reality for Geographic Visualization
Nick. R. Hedley, Mark Billinghurst, Lori Postner, Richard May Human Interface Technology Laboratory University of Washington
Box 352-142
Seattle, WA 98195, USA nix@u.washington.edu
Hirokazu Kato
Faculty of Information Sciences Hiroshima City University 3-4-1 Ozuka-Higashi, Asaminami-ku Hiroshima 731-3194, Japan kato@sys.im.hiroshima-cu.ac.jp
In this paper we describe two explorations in the use of hybrid user interfaces for collaborative geographic data visualization. Our first interface combines three technologies; Augmented Reality (AR), immersive Virtual Reality and computer vision based hand and object tracking. Wearing a lightweight display with camera attached, users can look at a real map and see three-dimensional virtual terrain models overlaid on the map. From this AR interface they can fly in and experience the model immersively, or use free hand gestures or physical markers to change the data representation. Building on this work, our second interface explores alternative interface techniques, including a zoomable user interface, paddle interactions and pen annotations. We describe the system hardware and software, and the implications for GIS and spatial science applications.
Keywords
Augmented Reality, Scientific Visualization, Computer Supported Collaborative Work.
1 INTRODUCTION
As computers become more ubiquitous they are increasing being used to support remote collaboration. Adding a camera and microphone to a computer converts it into a powerful tool for remote teleconferencing while software such as Microsoft’s NetMeeting allows users to share applications and effortlessly exchange data.
Although computer supported remote work is common, there are fewer interfaces for supporting face-to-face collaboration. This is particularly true for viewing and interacting with three dimensional data-sets and models. In this setting it is particularly important for the interface to provide intuitive viewing and manipulation of spatial models while at the same time supporting the normal conversational cues of a face-to-face meeting.
Augmented Reality (AR) is one particularly promising technology for face-to-face collaboration on spatial tasks. AR superimposes three-dimensional virtual images on the real world so users can see each other, the surrounding real environment, and the virtual models in their midst. Our previous research has found that users can collaborate more effectively in an AR setting than on the same task in an immersive VR interface [3], and that AR interfaces can enhance the cues already present in face-to-face collaboration [4].
One of the greatest benefits of AR interfaces is that they can be integrated into the existing workplace and combined with other more traditional interface technology. For example, the EMMIE system is a hybrid user interface that synergistically merges information in an AR headset with data shown on monitor and projection displays [6]. Within EMMIE users can seamlessly move virtual objects from the being overlaid on real world to being placed on a desktop monitor. Despite this seamlessness there are few examples of interfaces that combine AR techniques with other technologies.
In this paper we report on hybrid interfaces that combine technologies such as Augmented Reality, immersive Virtual Reality and computer-vision methods for natural user input. Our prototype interfaces are designed to allow multiple users to view and interact with geo-spatial data; however the techniques we describe could be applied to many different application domains. In fact, it is our hope that this work will motivate others to consider how AR technologies can be used in hybrid user interfaces for their own applications.
In the remainder of this work we first describe the application area in more depth, then each of the technologies used in our systems and the prototype hybrid interfaces developed around these technologies. We briefly present user experiences with

the system and contrast our work with those of other researchers in the field. Finally we outline our directions for future work and lessons we have learned from developing these interfaces.
2 APPLICATION DOMAIN: COLLABORATIVE GIS
Visualization of geo-spatial data is one of the most important application areas for virtual reality and computer graphics technology. Geographic Information Systems (GIS) facilitate powerful spatial analytical capabilities in academic, private, government and corporate arenas. These capabilities are largely a result of the analytical metaphors (e.g. Map overlay) and functionality facilitated by GIS software. Interface research during the past twenty years have brought us technologies that may fundamentally change the nature of how we interact with and understand spatial data. Advanced environments and tools render the interface far more transparent than ever before. Researchers have already shown the value of immersive Virtual Reality for data visualization [7][8] and science education [27]. These new technologies need to be investigated to evaluate their potential for integration into meaningful, valuable spatial applications. Geographers have been developing 3D GIS visualizations and moving independently of computer science and HCI, towards virtual environments for some time [13,22,28,29,35]. Convergences of computer science, HCI and geographic visualization have resulted in compelling collaborative 3D “geo-visualization” interfaces that represent a significant advance in GIS interfaces [12]. Initial efforts have been useful to inform researchers of methodological approaches to evaluating the effectiveness of collaborative GIS settings [32].
Advanced interface technologies, environments and associated modes of interaction are significant because of the way in which they may change the way in which we acquire spatial knowledge. There are practical and theoretical implications to the adoption of such visualization interfaces. Cartographers and spatial scientists need to reconsider the nature of human- visualization relationships as new visualization technologies emerge. This is especially important as these emergent technologies employ different mechanisms by which people interact with visual spatial information and acquire knowledge. This in turn requires us to evaluate the aspects of interfaces, visualization content and the individual users that influence the formation of spatial mental models. It is not suggesting that conceptualizations of mental models have changed, rather it is a question of whether the pathways through which they are formed has changed with the advent of these new interface technologies [14].
Linked to these altered relationships are significant questions that affect meaningful use and application of such tools. Do new visualization techniques match traditional forms of spatial visualization training, or do they engage different faculties of individuals? Does use of an AR 3D map (figure 1) improve performance in spatial decision-making over traditional 2D maps, or 2D and 3D GIS visualizations (figure 2)? Are the interactions different? What relative influence do 3D spatial model content, and interface type (AR, desktop, map) have over successful acquisition of spatial knowledge and collaborative spatial visualization activities? Do these factors impact the meaning of geographic visualizations and the information they present? Several parallel projects have begun to explore such questions, comparing AR versus desktop 3D spatial visualization use by individuals [14,15] and groups. These are intended to complement the project reported in this paper, as we develop a suite of AR interfaces, validation experiments, and conduct usability testing in applied settings.
Figure 1: AR Geographic visualization Figure 2: Desktop 3D GIS Geographic visualization output
From observing how GIS researchers use existing tools is it clear that any interface developed to support them must satisfy the following requirements:
• The ability to represent multi-attribute data.
• Support for multiple views into the same dataset, such as ego and exo-centric viewpoints. • The use of real tools and natural interaction metaphors.
   
• Support for a shared workspace.
It would be difficult for any single interface technology to satisfy all these requirements. For example AR interfaces can provide an exocentric view of a dataset and public and private information views, but it is difficult for them to provide egocentric views common in an immersive VR interface. For this reason, we believe that combining AR technology with other technologies in a hybrid user interface can more naturally support the GIS process. In the remainder of this paper we describe two hybrid user interfaces for GIS visualization; AR PRISM and its successor, the GI2VIZ interface.
3 The AR PRISM Interface
The AR PRISM interface is based on the underlying idea that multiple users gathered around a real map should be able to see and manipulate three-dimensional virtual geographic information superimposed on the map. This manipulation should be based on physical markers and tangible interaction techniques. To achieve this, AR PRISM uses a combination of three different interface technologies:
• Computer vision based AR tracking.
• Natural gesture-based interaction.
• Transitional interfaces.
3.1 Computer Visions Based AR Tracking
One of the most difficult parts of developing an Augmented Reality application is precisely calculating the user’s viewpoint in real time so that the virtual images are exactly aligned with real world objects. In our work we use ARToolKit, a free software library that uses computer vision techniques to calculate camera position and orientation relative to physical markers [19]. Once the real camera position is known, the virtual camera viewpoint can be set to the same position and computer graphics can be drawn that appear to exactly overlay the video image of the real marker. Rekimoto’s Cybercode software [34] also uses computer vision techniques to provide a similar capability.
Users typically wear a lightweight head mounted display (HMD) with a small video camera attached. The HMD and camera are connected to a computer that performs image processing on the video input and composes computer graphics onto the video image for display in the HMD. When users look at a tracking marker they see virtual images overlaid on the marker in a video see-through augmented reality. This ability to overlay virtual models on real physical objects naturally lends itself to physically based interaction metaphors, such as virtual interactions based on real object proximity, or placement.
3.2 Natural Gesture Interaction
A second interface technique used is wireless natural gesture interaction. This enables users to interact with datasets using natural hand gestures or physical object motion. To accomplish this we use a projection table and an overhead camera. The surface of the table is made of opaque glass and beneath the table there are several lamps that produce a large amount of Infra-Red (IR) light. Above the table is a fixed camera with a filter over its lens that filters out everything but IR light (figure 3).
Figure 3. Rear-Projection Display Table
When users stand around the table and place their hands or other objects on the table surface these create dark patches in the IR image seen by the overhead camera (figure 4). This image is passed to a computer connected to the camera where
 
background subtraction and edge detection routines are used to find the outlines of hands and objects on the table (figure 5,6). The hand shape and position, and other object position can then be used to change application parameters or as input channels. This software can track as many hands and objects as there are on the table, enabling several users to gather about the table and interact with the dataset. For a complete description of how this software works please see [30].
   Fig 4: Input IR Image Fig 5: Subtracted Image Fig 6:Edge Detection
3.3 Transitional Interfaces
The AR PRISM interface uses techniques developed in the MagicBook project [5] to allow users to smoothly move between Physical Reality, Augmented Reality and immersive Virtual Reality. Given that different degrees of immersion may be useful for different visualization tasks, we believe that a transitional interface has the flexibility to meet the changing needs of users throughout the GIS collaborative process.
In the AR PRISM interface if several users are gathered about the real map and looking at the same tracking marker they will see an AR image from their own exocentric viewpoint. When a user finds an AR model interesting they can fly into the virtual world and experience it immersively, freely moving through the world and looking about it from an egocentric viewpoint. Users immersed in the virtual world will see each other represented as virtual avatars. More interestingly, if one user is viewing the computer graphics in an AR mode while another is in immersive VR, the AR user will see an exocentric view of the avatar of the immersive user in the virtual world (figure 7).
AVATAR
Fig 7: Avatar in an Exocentric AR View
Thus, the AR PRISM interface supports collaboration on three levels:
Level 1 As a Physical Object: Users can view and interact with the physical objects in a face-to-face setting.
Level 2 As an AR Object: Users with AR displays can see virtual objects appearing on the physical objects.
Level 3 As an Immersive Virtual Space: Users can fly into the virtual space together and see each other represented as virtual avatars in the VR scene.
3.4 The hybrid Interface
In the final hybrid AR PRISM interface users stand around the projection table with a real map spread out on top of it and several physical tracking markers (fig 8). They each wear or hold a lightweight HMD with a small video camera attached. Each camera is connected to an SGI O2 computer and the video output from the O2 is fed back into the headset, enabling the user to experience video see-through AR. A separate overhead camera connected to a Pentium PC with a Matrox video capture board is used to track hand and object position on the table. The SGI O2’s and Pentium PCs are networked to
   
exchange tracking information. The network bandwidth between the machines is very small so the number of users could scale to as many as dozens around the same table.
Fig 8: Using the AR PRISM Interface
There are two different display options. Users can either wear a pair of Olympus Eyetrek displays (figure 8), or they can hold a handle with a Sony Glasstron mounted on it (figure 9). Both displays are full color with NTSC resolution, and are bi-ocular rather than stereoscopic. Wearing the Eyetrek has the advantage that both the users hands are free, while the Glasstron is easy to remove from the face and pass among several users. The Glasstron configuration is also designed to support transitions between the AR and VR scenes. To do this, the Glasstron handle has an InterTrax30 inertial tracker [17] mounted on it and a pressure pad and switch. The inertial tracker provides head orientation information when the user is immersed in the virtual scene. Flicking the switch causes the user to fly in or out of the virtual scene while pressing on the pressure pad moves them through the immersive world in the direction they’re looking.
Fig 9: The Sony Glasstron Mount
3.5 AR PRISM Software
The AR PRISM software is made up of two different components; a hand/object tracker and several AR graphical clients (one for each user). The hand/object tracker is run as a simple sever application, broadcasting output to any of the AR graphical clients connected to it. The output is simply in the form: <EventType> <xPosition> <yPosition>
EventType is an integer specifying if the position information is related to a hand or object position, while xPosition and yPosition are the normalized coordinates of the hand tip of marker center point on the table. So if a user placed a marker object in the center of the table the following message would be sent to all the connected clients: “2 0.5 0.5”
Each of the SGI O2s runs the AR PRISM graphical client. This software integrates the ARToolKit tracking library with a VRML97 parser to enable any VRML content to be viewed on a physical marker. The VRML 97 parser is based on the open source libVRML97 library [26] with modifications made to allow ARToolKit to set camera viewpoint information.
The actual content shown in the displays depends on the messages send from the overhead camera and PC server. For example if a physical marker is placed in the middle of the map then the (x,y) coordinates of the marker (in this case (0.5,0.5)
  
are used in a lookup table to load the VRML model corresponding to the 3D virtual terrain model for that portion of the real map. In this way there can be multiple VRML models associated with each physical marker.
3.6 User Interactions
The basic interface metaphor we developed for this application was a “Sensor-Data” metaphor. That is, there is a physical marker (the terrain marker) representing the portion of the dataset being viewed and other markers representing “sensors” that modify the way this data is represented. For every grid location in the real map there was three-dimensional topographic information available, and for some locations there was also soil, hydrology and well information.
As the user moves their hand across the surface of the real map in their headset display is shown a graphic overlay of longitude and latitude information for where there hand was and icons representing data available at that point. If the user wants to view some of the geological dataset they place a terrain marker on the map (figure 10). Once the user has removed his/her hand the hand/object tracking system determines that a marker has been placed on the map and sends the coordinates to the AR client software. The AR software then recognizes the marker and looks up its map location to retrieve the correct VRML model and render it over the marker card (figure 11).
  Fig 10: User Placing a Terrain Marker Fig 11: VRML Model of Terrain
If the user wants to see a new portion of the virtual terrain they simply move the marker to a new map location, the system recognizes the new location and swaps VRML models (see Figure 12). The AR scene and real map are aligned so that features seen on the map can be seen in the three-dimensional virtual model overlaying the same map location. So in figure 12 the coastline in the virtual model is aligned with the coastline on the real map. Once the terrain model has been loaded there is no need to keep the marker on the map. The user can look at the visualization from any angle and at any distance (figure 13). The card can be picked up so that the user can hold it in his/her hand, rotate it, and see more detail. Each of the users about the table can see the terrain model from their own viewpoint and the model can even be picked up and passed around the users.
  Fig 12: Updated VRML Mode Fig 13: User Getting Close to Terrain Model
Many of the map grid locations have content other that just topography. If the icons in the display indicates there is hydrology, soil or well information available the user is able to see this by using the appropriate sensor card. There is one physical marker for each type of data sensor. Placing this marker next to the terrain marker causes the VRML model to change to the appropriate data set. For example, placing the soil sensor next to the terrain marker changes the texture on the terrain model to show soil features (figure 14a). This swapping is based on physical proximity of the data and sensor cards.

                                 3.7 User Experiences
Before
After
Fig 14: (a) Loading Soil Data (b) Subsurface geological model
Viewing the soil information just changes the model texture, however entirely new models can also be loaded. For example, when the user places the well sensor card beside the terrain model the model becomes a volumetric display of subsurface features including vertical wells (figure 14b).
Once a terrain model has been loaded the interactions are all performed using the AR client software, so the use does not have to place both cards on the map for this data swapping to take place. The views into the dataset are also independent, so if one use wants to see the hydrology information of a particular region they can pick up the terrain card, place the hydrology sensor card next to it and see the hydrology model loaded. However, if another user does not see those two cards together their AR client will not load the hydrology dataset, enabling them to keep on viewing the original terrain model.
Users can also fly into a scene and experience it immersively. Once they have selected the region of the map they are interested and loaded the AR scene, if they flip they switch on the Glasstron handle the camera viewpoint smoothly animates from being an AR exocentric view to an immersive VR egocentric view (figure 15). The ARToolKit tracking is now turned off and the inertial tracker is used to allow the user to freely look around in any direction. Pushing the pressure pad flies them in the direction they’re looking, removing the need for them to change their physical position in the real world. As described previously users not immersed in the VR scene can see a miniature virtual avatar of the VR user, or they can fly into the virtual scene themselves see each represented as life-sized avatars.
AR View Immersive VR View
Fig 15: Transition from AR to VR
We are in the process of conducting formal user studies, although almost one hundred people have tried the AR PRISM software informally. These users have generally found the software very easy to use. Associating a virtual dataset with a physical object means that this data is as easy to manipulate as the real object. They also like the ability to see other users in the real world at the same time as seeing the AR model, and felt that this supports natural face to face collaboration. Generally users preferred the Sony Glasstron display mount to the hands-free Olympic Eyetrek configuration. This was not only because it was easy to share views with others, but also they reduced their feeling of separation from the real world and reduced incidences of nausea.
There is some room for improvement though. Almost all the users that fly into immersive VR scenes ask how they can move backwards in the scene. When we developed the interface we assumed that we should mimic the real world where people rarely walk backwards, or in the opposite direction to which they are facing. However in most first person computer games and VR environments this is common, so users often ask for it. Users also needed some reminding not to cover tracking pattern and sometimes had to be shown how to orient the physical markers for best tracking results. This implies that we may

 want to spend more time on developing a physical marker with a form factor that makes it difficult to obscure the tracking pattern.
3.8 AR PRISM Content
Although this interface could be used for interacting with any geological datasets the first models we content we have used is from the Big Beef Creek in the Puget Sound Region of Washington State, USA (figure 16). The Big Beef Creek Field Station is run by the University of Washington's School of Fisheries and is used to investigate salmon propagation (among other things). The School of Fisheries has detailed topological data of the region as well as data about soil types, hydrology and environmental features.
Fig 17: Big Beef Creek Fig 18: Base Map
In order to use this content a 2D base map of the area was divided into 35 grid cells (7 rows and 5 columns) (figure 17) and VRML models generated for each of the cells. These models were generated from USGS digital elevation model (DEM) data, using the IRIS Explorer data visualization application.
4 RECENT WORK: THE GI2VIZ INTERFACE
Although the AR PRISM interface provides an intuitive way to collaboratively view geo-spatial data it has a number of limitations. Firstly, there is limited support for interaction with the data set; users cannot annotate the data or place their own virtual models into the AR scene. There is also no support for 2D imagery in the application. In many GIS applications 2D images such as satellite photos can be as important as the 3D terrain models. A drawback with the ARToolKit tracking technique is that is uses a single tracking marker, meaning that virtual models can disappear if this marker is even partially obscured. Finally, the use of a real map prevents the zooming and panning techniques possible with a projected map image.
To address some of these limitations and to extend the AR PRISM interface we are working on its successor, the GI2VIZ interface. The GI2VIZ project is a hybrid interface that has many of the same characteristics of the AR PRISM interface. In addition it supports the use of projected 2D imagery, allows annotation of both 2D and 3D content, allows manipulation of the virtual models and addition of content to the virtual scenes, and supports a new, more robust multiple marker AR tracking technique. In this section we briefly describe these enhancements.
Like AR PRISM, the GI2VIZ interface is based around a projection table. However in this case 2D map imagery is projected onto the surface of the table (figure 19). The glass table also has a MIMIO input device mounted on it. The MIMIO [31] is a commercial device that uses ultrasonic and infrared signals to track pen input on a flat surface. The MIMIO pen can be used for normal mouse input in any Windows PC application. The application run on the projection table supports loading and viewing of 2D map images. It also has a zoomable user interface based on the Pad++ work [1]. This allows the user to pan around the map images and zoom into any arbitrary resolution using the MIMIO pen. Some of the 2D images also have square ARToolKit tracking markers embedded in them (figure 20). When the user looks at these marks through their headset displays they see virtual 3D terrain imagery overlaid on the projected 2D image. In this way the GI2VIZ interface supports simultaneous viewing of 2D and 3D images.

  Figure 19: GI2VIZ Interface Figure 20: Virtual Model Overlay
There is also a connection between the 2D map images and the 3D content. The user can use the MIMIO pen to annotate the 2D projected imagery. As they draw lines on the 2D images, virtual lines appear on the associated 3D VRML models. These drop lines follow the contours of the model and provide an easy way for users to mark regions of significance in the virtual dataset (see figure 21).
Figure 21: VRML Terrain Model with Annotations.
The ARToolKit library has also been enhanced to support additional tangible interaction techniques. The basic ARToolKit library was extended to support camera pose determination from sets of tracking markers. This increases the robustness of the AR tracking and means that one or more of the markers can be occluded and the camera pose still found. See [ref] for complete details. Figure 20 shows a virtual model overlaying a set of projected marks. This enhancement also enables the use of a wider range of tangible interaction techniques. One of the first we have been exploring is a real paddle that can be used to manipulate virtual objects. The paddle is marked with a tracking marker that enables it to be tracked relative to the set of markers that are overlaid with a virtual terrain model. The position and orientation of the paddle can be found, so a number of tangible interaction techniques can be used based on paddle tilt, motion and proximity to virtual objects. Users can pick up virtual models by placing the paddle over them and then drop the models onto the virtual terrain dataset (see figure 22). Once placed on the terrain simple collision detection is used to allow the user to push the models around with the paddle. In this way users can quickly add objects to the geo-spatial AR scenes. This paddle interaction technique is based on earlier work presented at ISAR 2000 [20] but modified to work within the GI2VIZ hybrid interface.
 
 5 RELATED WORK
Figure 22: Paddle interaction with the AR scene.
A number of researchers have developed interfaces that we can compare our work to, although none have presented a hybrid interface with the same combination of technologies.
There are many examples of augmented tabletop interfaces. Some, such as the Perceptive Workbench work of Leibe et. al. [25] use infrared sources to aid in hand tracking, while others such as the pioneering work of Krueger’s VIDEODESK [23] use visible light cameras. Projection tables such as the Responsive Workbench [24] are capable of showing stereoscopic virtual images, and typically use magnetically tracked input devices to interact with these images. Other systems, such as Ullmer’s MetaDesk [37] support the use of real object as interaction devices. Our work is related to these efforts, but different in a number of keys ways:
• The use of AR displays allows it to support an arbitrary number of face-to-face collaborators.
• In the AR PRISM interface the table surface is covered by a physical map and just used for interaction, not projected
display.
• The physical props are not only interaction devices but are also used as containers for virtual data.
The use of physical objects as input devices builds on the work of Hiroshi Ishii’s Tangible Media group [36]. The many projects developed by this group are based around the notion of the Tangible User Interface (TUI) in which real world objects are used as computer input and output devices, or as Ishii puts it “by coupling digital information to everyday physical objects and environments” [18]. As Fitzmaurice points out, Tangible interfaces are extremely intuitive to use because physical object manipulations are mapped one-to-one to virtual object operations, and they follow a space-multiplexed input design [9].
Although intuitive to use, with Tangible interfaces information display can be a challenge. For example, in the Triangles work [11], physical triangles are assembled to tell stories, but the visual representations of the stories are shown on a separate monitor distinct from the physical interface. Presentation and manipulation of 3D virtual objects on projection surfaces is difficult [10], particularly when trying to support multiple users each with independent viewpoints. Most importantly, because the information display is limited to a projection surface, users are not able to pick virtual images off the surface and manipulate them in 3D space as they would a real object.
The AR PRISM interface overcomes these problems by combining the enhanced display possibilities of Augmented Reality with the intuitive manipulation of Tangible User Interfaces. We call this combination Tangible Augmented Reality. Earlier AR interfaces used more traditional input devices, such as a hand-held mouse or tablet [33][16], or magnetic trackers to allow people to view and edit AR content [21]. In many cases these interfaces used existing 2D and 3D interface metaphors from desktop or immersive virtual environments. As can be seen from our work, Tangible AR interfaces are anchored in the real world and AR content manipulation is based on interaction between real world objects. This means that Tangible AR interfaces are as easy and intuitive to use, as it is to manipulate real objects.
6 CONCLUSIONS AND FUTURE WORK
In this paper we have described how we have combined different interface technologies to create two hybrid interfaces for geo-spatial data visualization. Combining AR and VR techniques we have created an interface where users can seamlessly move between the real and immersive virtual worlds. Combining computer vision hand and object tracking with AR techniques we have produced an intuitive means for loading datasets into the AR environment. Adding support for pen-based input and 2D image viewing we have enabled the user to make annotations on the 3D virtual models.

In the future we will explore more interesting interaction techniques that go beyond model loading based on physical marker proximity or navigation in VR scenes. For example if we have an AR model of a virtual column of data appearing above a card, then we should be able to move a second card up and down this model to view different aspects of this data-set using a variation on the “Magic-Lens” technique [2]. Other interaction techniques to explore include being able to show layers of information (for example, weather on top of terrain), and the addition of speech input for true multi-modal commands.
Finally we will develop techniques for feeding live data into the interface. So for example, as weather patterns change the weather texture maps will be updated in near real time. This will also enable this interface to be used to emergency management and other command and control type application where is it critical to know where real vehicles and people are in remote environment at all times.