Multi-User Egocentric Online System for Unsupervised Assistance on Object Usage
Dima Damen, Osian Haines, Teesid Leelasawassuk, Andrew Calway and Walterio Mayol-Cuevas
Department of Computer Science, University of Bristol, Bristol, UK
Abstract. We present an online fully unsupervised approach for auto- matically extracting video guides of how objects are used from wearable gaze trackers worn by multiple users. Given egocentric video and eye gaze from multiple users performing tasks, the system discovers task-relevant objects and automatically extracts guidance videos on how these objects have been used. In the assistive mode, the paper proposes a method for selecting a suitable video guide to be displayed to a novice user indi- cating how to use an object, purely triggered by the user’s gaze. The approach is tested on a variety of daily tasks ranging from opening a door, to preparing coffee and operating a gym machine.
Keywords: Video Guidance, Wearable Computing, Real-time Computer Vision, Assistive Computing, Object Discovery, Object Usage
1 Introduction
With the advent of wearable devices, systems able to provide guidance to users remain a possibility and a challenge. In particular in industrial settings (e.g. as- sembly, repair), operations using augmented reality or video-based manuals have been promised for a while. One of the key limitations to realize such systems is the need for authoring the content by e.g. manually segmenting and annotat- ing videos or creating three-dimensional models that represent meaningful guid- ance [16],[1]. Authoring is time consuming and evidently limiting. Approaches that can provide guidance without the need for any manual intervention would enable a wider adoption of assistive wearable systems.
In this paper we present a fully automated, online and real-time approach for providing video-based guidance on object usage from egocentric video and eye gaze. The system has two modes, a learning mode where video snippets are automatically extracted from videos of multiple users performing tasks around a shared environment, and an assistive mode where a ‘suitable’ video snippet from the automatically collected video guides is selected, triggered by gaze. In strong contrast to most previous work on assistive egocentric guidance, we require no pre-training of the objects involved in tasks, nor knowledge of the tasks’ scripts or the knowledge of how many objects will be used or interacted with. The approach is able to harvest video snippets for objects of interest as a precursor for cognitive assistance. The system selects a short assistive snippet or video

2 Damen et al.
guide to be shown when a gazed-at object is recognised, to illustrate how the object was used before. This paper presents a prototype for the system and concentrates on evaluating the extraction of objects and their use. We illustrate the annotation of test videos with the automatically extracted video guides1, and leave the evaluation of the effectiveness of the assistive mode with real users for future work.
The setup uses a single wearable gaze-tracker eyepiece which features a cam- era that looks out towards the scene and a pupil tracker that indicates where the eye is looking.
2 Related Work
Related systems to the problem we are aiming to address expect the objects to have visual markers (e.g. [18]), use model-based tracking(e.g. [16]) or be specified in advance of task performance (e.g. [1]). This review focuses on the ability to find objects of interest, i.e. task-relevant objects (TRO), from egocen- tric video during task performance. Common approaches include i) segmenting the area surrounding the user’s hand [7],[6],[12], ii) extracting foreground re- gions through frame stabilisation or scene planarity assumptions [17],[21] or iii) detecting ‘object-like’ regions [15].
One uniquely rich source of information in egocentric sensing is eye gaze. Eye gaze has been studied for hundreds of years and more intensively since the 19th century [23]. There are two principal eye behaviours: fast motion transitions (aka saccades) and eye fixations. Importantly, studies of eye fixations during everyday tasks show substantial similarities in the locations and number of fixations by different operators, that gaze rarely visits irrelevant objects and that fixations precede actions [11],[8].
However, eye gaze has been rarely considered as part of wearable systems, perhaps due to the scarcity of mobile gaze tracking hardware. Exceptions include [5] which exemplifies how gaze can assist in predicting the current action, and how the predicted action can be used to estimate the forthcoming gaze position. In [20], a wearable gaze-controlled camera provides a cropped image dictated by eye gaze locations to enhance object tracking and in [4], interest points are ex- tracted around the gaze point and matched to pre-learnt highly textured objects. None of these approaches discover objects using gaze. In [14], object segmenta- tion using gaze is attempted from annotated short clips containing action, though the work focuses on gaze estimation.
Our recent work [3] has compared the influence of gaze, position, appearance and motion using offline processing on the extraction of objects in egocentric video. Results prove that 80% of objects were correctly extracted by localis- ing gaze within a 3D map. In this work, we use the same dataset but propose an online incremental algorithm that learns objects and extracts video help guides incrementally from multiple operators. An online approach would scale with more users without the need for re-training, and data can be processed on
 1 http://www.cs.bris.ac.uk/~damen/You-Do-I-Learn

Multi-User Egocentric Online Sys. for Unsupervised Assistance 3
the fly avoiding the need to store lengthy hours of egocentric video collected from multiple users. To enable real-time processing, we learn objects using the shape-based real-time object learning and detection method [2], which is capa- ble of accommodating multiple objects in a scalable manner using constellations of edgelets. The algorithm is also capable of detecting ‘moveable’ objects and distinguishing them from ‘static’ objects that remain fixed in the 3D map.
3 Proposed Method
Our method is based on four principles:
– Spatio-temporally consistent gaze fixations indicate an observation of a task- relevant object (TRO).
– Each observation represents a candidate video snippet for assistive guidance.
– Spatially consistent observations correspond to a fixed TRO (i.e. an object
with a fixed location in the scene).
– Appearance-consistent observations, observed in different locations, corre-
spond to a moveable TRO.
The input to the system is real-time egocentric video with 2D gaze fixations. In the learning mode (Sec 3.1), the system aims to learn objects of interest as well as extract video snippets on how these objects are used. In the assistive mode (Sec 3.2), the system aims to recognise gazed-at objects and select a suitable video snippet for guidance from the automatically extracted snippets in the learning mode. The approach is completely unsupervised, and details of both modes are discussed next.
3.1 Learning Mode
First, we follow the velocity-based approach from [19] to distinguish saccades from fixations, and position the 2D fixation relative to the scene using sparse Simultaneous Localisation and Mapping (SLAM) [9]. Given the 6D pose of the scene camera, a 3D gaze ray links the direction of the gaze to a point in the scene. A dense depth map is estimated, using a triangular tessellation on the tracked interest points that are visible on the scene camera (similar to [22]). To distinguish between the 2D fixation at time t and its corresponding 3D position within the map, we refer to these as ft2 and ft3 respectively.
Next, objects are discovered using online clustering, as explained below and
in Algo. 1. We define a gaze cluster (GC) as a collection of ‘at least’ ξ spatially-
close consecutive gaze fixations, and use this to learn objects. Two consecutive
fixations, f3 and f3 belong to the same GC if ||f3 − f3 || < ǫ, where ǫ is t t−1 t t−1
the distance threshold selected to accept clustering consecutive fixations and ||.|| is the Euclidean distance. Notice that the temporal difference between t and t − 1 might not correspond to one frame, as some frames have missing gaze information, or the gaze might have been discarded as a saccade. If and only

4 Damen et al.
   1 2 3 4 5
6 7
8
 9
10
11
12
13
14
15
16 17
18 19 20 21 22
23 24 25
26 27 28
29
input : fixations {(ft2 , ft3 )}, images {It }; t = 1..T output: TROs {(Ak, Uk, mk, νk); 1 ≤ k ≤ K}
Ak learnt view-based appearance model for TRO i Uk video snippets for TRO i
νk ∈ {fixed, moveable} type of TRO mk segmented 3D model for TRO k
K = previousK = 0 stableGC = 0 fort=1..T do
find closest gaze cluster k: min argk ||ft3 − μk ||Σk Extract window wt centred around ft2 from It
// Object Discovery
if||ft3−μk||Σk ≤1then Update μk(Eq 1),Σk(Eq 2)
   else
if ||f3 −f3 ||<ǫthen
 t t−1
stableGC = stableGC + 1 if stableGC ≥ ξ then
  K=K+1
Add a new gaze cluster k = K Learn the first view of a new object νk = ‘fixed’
  else
stableGC = 0
// Learn Appearance
Detect an object within the window wt if recognised as TRO j then
if j ̸= k then
if confirmed from several detections then
νk = ‘moveable’
if Object was not detected in last δ frames then
Learn a new view for object k
// Video Snippets and Model
if k ̸= previousK then
add video snippet uki to Uk (Eq. 3) build 3D model mk
// Keep track of current GC
previousK = k
         else
        Algorithm 1: Proposed algorithm for learning mode

Multi-User Egocentric Online Sys. for Unsupervised Assistance 5
 Fig. 1. Two TROs were discovered (a,b). Later, the tape was moved (c). A new fixation is spatially close to TRO ‘0’ (d). Initially, further views were collected for TRO ‘0’ (d,e). A few frames later, the object is consistently recognised as TRO ‘5’ by appearance matching. Both TRO ‘0’ and ‘5’ are marked as ‘moveable’ (f).
if ξ consecutive fixations are within the same GC, an observation of a TRO k is discovered (Algo. 1 L. 9-15). The mean and covariance of GC are updated incrementally as further fixations are located within the threshold ǫ. Equations 1 and 2 show the incremental update for the mean and covariance of a GC.
3 3 k μ kt − 1 × ( n − 1 ) + f t 3
||ft −ft−1||<ǫ→μt =
→Σk = n−2Σk
(1) + 1(f3 −μ )T(f3 −μ ) (2)
 n
t n−1t−1 nt n−1 t n−1
  where μkt is the mean, Σtk is the covariance matrix and n is the number of clustered fixations at time t.
Attention is believed to have moved to another location when ||f 3 −f 3 || ≥ ǫ. t t−1
At a future point in time t+ρ, further fixations can belong to the same TRO k if it is within one standard deviation from the mean of the TRO k according to the Mahalanobis distance (Algo. 1 L. 6-7). This clustering enables both small-sized and large TROs to be discovered, as it does not limit or pre-define the size of the GC. However, it assumes that the object is fixed, i.e. remains within the same 3D location.
To accommodate for moveable objects, appearance matching is considered. For every TRO k, views around the object are learnt using the real-time method from [2]. Only novel views are added to the appearance model - a view is added if the object fails to be recognised in the past δ frames (Algo. 1 L. 24-25). The gazed-at object is compared to the previously learnt K objects {Ak;k = 1..K}. If the appearance matches a learnt TRO, at a different location, the object is

6 Damen et al.
believed to have moved, and is thus identified as a ‘moveable’ object (Algo. 1 L. 19-22). To avoid incorrect detections, multiple consecutive matching appearances are required before an object is identified as ‘moveable’. Figure 1 shows an example of identifying a ‘moveable’ object.
Notice that identifying an object as ‘moveable’ could result from multiple instances of the same object. A limitation of the approach arises when a new object replaces a learnt TRO. The object is then incorrectly learnt as novel views of the previously learnt TRO. This does not affect the assistive nature of the method, as we use the current object’s appearance to select a suitable help snippet as will be explained next.
As we position gaze in 3D space, we can exploit this information to generate visualisations of the TROs as a byproduct of the process (Algo. 1 L. 28). This step adapts [13] so it does not require the detection of keyframes form the user’s motion and does not assume a single user is providing input. Despite not being perfect models, due to the fact that they are created during an action, the result- ing models are useful visualisations of what objects the system has discovered. Ultimately, having a 3D model facilitates applications such as augmented reality guidance which we leave for future work.
Given consecutive fixations (f2,f2 ,...,f2 );ρ ≥ ξ belonging to the same t t+1 t+ρ
TRO, a video snippet uki for TRO k is defined as
uki = {Ψ(Ij,∆(j),ω) (3)
where Ψ crops a window of size ω from Image Ij around the interpolated fixation ∆(j) as gaze information is missing in some frames (Algo. 1 L. 27). The collection of all video snippets Uk shows different ways in which the object k was used or interacted with.
As multiple operators with different heights and interaction behaviours use the same object, the method is capable of expanding the learnt views, the 3D model mk and gather further interaction video snippets Uk. Figure 2 shows the advantages of learning from multiple users.
Fig.2. For the same discovered object (sink): multiple users enable learning varying views in the appearance model Ak (left); the 3D model mk (middle) is refined (m1k) shows the model for one user, two users (m2k) as well as five users (m5k); different video snippets Uk show multiple interactions with the same object (right).
 
Multi-User Egocentric Online Sys. for Unsupervised Assistance 7
 Fig. 3. During discovery (left), edges within a window around the gaze are captured as object views, and represented using affine-invariant descriptors. These are used to detect objects around the gaze point in real-time (right).
3.2 Assistive Mode:
In the assistive mode, video snippets {Uk;k = 1..K} can be used to provide automatic assistance for novice operators. First, the system needs to identify which object the person intends to use next, then the system would select a video snippet, from the potentially many snippets collected from multiple operators using the object one or more times, to be displayed to the novice operator.
We recognise objects based on the learnt views in an image patch around the gaze point using the scalable real-time texture-minimal object detector from [2]. By using the combination of fixed paths and a hierarchical hash table, the method is scalable, and can reliably detect objects at frame rate. The descrip- tor is affine-invariant, and the method is tolerant to a level of occlusion but is also view-dependant. Figure 3 shows the method learning (left column) and subsequently recognising (right column) objects from our experiments. Notice that the assistive mode does not require 3D tracking, and objects are recognised around the 2D gaze point.
Upon recognition, a help snippet is displayed to show how this object was previously used. From the possibly many video snippets featuring the TRO, collected in learning mode, we chose the help snippet ht as a video guide at time t such that the appearance of the first frame in the snippet, is closest to the recognised view. If the object changes state, the initial appearance is a good indicator of which video snippet to show. An additional advantage is to avoid showing a snippet observing the object from a different viewpoint, so the user can easily map what they see to what they could do.
A help snippet is displayed each time a new object is detected. As some objects can be gazed-at multiple times during the task, we employ temporal

8 Damen et al.
  1 0.8 0.6 0.4 0.2
0
0 0.2 0.4 0.6 0.8 1
Recall
        # of learnt views (ε=0.2 varying ε
data3
data4
)
           Fig.4. Precision-Recall curve for discovering TROs as ǫ (Eq. 1) varies. For ǫ=0.2 metres, discovered objects are filtered based on the number of learnt views - at 76% recall, 100% precision was achieved.
ordering in choosing the help snippet. That is, for a given object we choose its snippets in order, starting first from all the first encounters of that object in all training sequences. When the same object is gazed-at again, a snippet from the set from all the second encounters in the training sequences is displayed and so on.
4 Experiments and Results
Setup & Dataset We use the dataset from [3] which was recorded using the wearable gaze tracker hardware [10]. After calibration, the scene images are synchronised with, if available, 2D gaze points. Twenty objects were ground- truthed, of which 5 are moveable objects.
To evaluate the ability of online clustering to find TROs, a 3D bounding box around fixations from one discovered TRO is compared to the manually labelled 3D bounding box on the map’s point cloud. The PASCAL overlap criterion (adapted for 3D) of 20% is used for a true positive discovery, using the algorithm detailed above (parameter choices in Tab. 1). The main parameter for clustering is the threshold for 3D distances (ǫ). As ǫ (Eq. 1) varies, the number of discovered objects changes. The recall-precision results are shown in Fig. 4.
For ǫ = 0.2, Tab. 2 shows the mean and standard deviation for the number of discovered, merged and split objects in one and all sequences. Since the cluster- ing is online, different runs would result in a different set of discovered objects depending on the ordering of sequences. We run the experiments multiple times (5 times), starting from a different sequence, and record the results. As the table
   Eq. 1
 Algo. 1
  k
10 frames
   δ
 5 frames
  ǫ
20 cm
   w
 150 × 150
    Table 1. Parameter choices for object discovery
Precision

Multi-User Egocentric Online Sys. for Unsupervised Assistance 9
  W
P
                D
Fig. 5. [Best seen in colour] 22 objects were discovered within the four maps (W, D, P and K) listed from left to right by order of discovery. Out of these, 14 ground-truth objects are found (with 3 splits), and 5 are task-irrelevant (red rectangles). The ‘cup’ was missed at this iteration. Three objects were classified as ‘moveable’ (blue ellipses), out of possible 4. The charger was discovered twice and the sugar jar was discovered as three different objects.
shows, when trained using a single operator, precision of 79% is achieved along- side 86% of recall. When training on all operators, on average, 97% recall was achieved, with an increase in the total number of discovered objects. The num- ber of false positives can be dropped by filtering for the number of learnt views, as operators observe TROs for longer than other objects in the scene (Fig. 4). The approach also separates fixed from moveable TROs. Recall that a TRO is ‘moveable’ if it is detected in different locations, using appearance matching. On average, 77% of TROs were correctly classified. The set of discovered objects from a single run is shown in Fig 5. Examples of learnt views for the discovered objects can be found in Fig 6.
Assistive Mode: To assess the ability of the approach to provide video guides, the approach is run using leave-one-out. For every operator, the learning mode is run on the remainder sequences to discover TROs and collect video guides. The appearance models of discovered TROs are then used to recognise objects in the ‘left’ sequence (i.e. not used for discovery), within patches around the 2D gaze. When an object is recognised, an insert is added indicating a suggestive way of how the object can be used. A help snippet ht is displayed each time a new object is recognised. We showcase video help guides using inserts on a pre-
Table 2. At ǫ = 0.2 metres, from one and all operators, the avg. (μ) and std dev. (σ) of the # of discovered TROs, the # of true TROs (ground-truth=20), the # of merged objects (ground-truthed as two separate objects), the # of split objects. For distinguishing moveable from fixed objects, the # of correctly classified objects.
K
                Op
   total
gt TROs
 merged
  split
 type
 1
   μ
 21.6
17.2
 0.7
  1.5
 -
  σ
 1.5
0.9
 0.5
  0.8
 -
 All
   μ
 33.2
19.3
 0.2
  3.8
 15.5
  σ
 2.0
0.8
 0.4
  1.6
 0.9
      
10 Damen et al.
 Fig. 6. Learnt views from training sequences of multiple users for a variety of objects: coffee machine, tap, seat adjustor and screwdriver.
    Fig.7. In the assistive mode, when a TRO is detected, a video snippet is inserted showing the most relevant video guide based on the initial appearance.
recorded video. Figure 7 shows frames from the help videos and a full sequence is provided2. Recall that these inserts are extracted, selected and shown fully automatically. These could in principle be shown on a head-mounted display, but is not considered in this study. We believe this highlights the success and potentials of the work in this paper.
5 Conclusions and Future Work
In this paper we develop an online real-time system based on egocentric video with gaze. In its learning mode, the system discovers task-relevant objects and automatically collects video snippets from multiple users on how they used the discovered object. In the assistive mode, video guides are shown on how objects have been used before, triggered by recognising the gazed-at object. This could
 2 http://www.cs.bris.ac.uk/~damen/You-Do-I-Learn

Multi-User Egocentric Online Sys. for Unsupervised Assistance 11
be useful to novice users exploring the same environment and objects. This paper explains a complete online prototype, and future work aims to evaluate the benefits of the assistive mode on the performance of novice users.